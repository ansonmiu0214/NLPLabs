{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download WikiText-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import operator\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# some helper functions\n",
    "def prepare_data(filename):\n",
    "    data = [l.strip().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
    "    corpus = flatten(data)\n",
    "    vocab = set(corpus)\n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram LM\n",
    "An language model assign a probability to each possible next word given a history of previous words (context). \n",
    "\n",
    "$P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$\n",
    "\n",
    "### Markov Assumption\n",
    "Since calculating the probability of the whole sentence is not feasible, the Markov Assumption is introduced.\n",
    "\n",
    "It assumes that each next word only depend on the previous K words (In an NGram language model, K = N-1).\n",
    "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
    "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
    "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
    "\n",
    "For a NGram language model, the probability is calculating by counting the frequency:\n",
    "\n",
    "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM():\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.vocab = set()\n",
    "        self.data = []\n",
    "        self.prob = {}\n",
    "        self.count = defaultdict(Counter)\n",
    "    \n",
    "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
    "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
    "    def train(self, vocab, data, smoothing_k=0):\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.smoothing_k = smoothing_k\n",
    "\n",
    "        if self.N == 1:\n",
    "            self.counts = Counter(flatten(data))\n",
    "            self.prob = self.get_prob(self.counts)\n",
    "        else:\n",
    "            self.vocab.add('<s>')\n",
    "            counts = self.count_ngram()\n",
    "            \n",
    "            self.prob = {}\n",
    "            for context, counter in counts.items():\n",
    "                self.prob[context] = self.get_prob(counter)\n",
    "    \n",
    "    def count_ngram(self):\n",
    "        counts = defaultdict(Counter)\n",
    "        for sentence in self.data:\n",
    "            sentence = (self.N - 1) * ['<s>'] + sentence \n",
    "            for i in range(len(sentence)-self.N+1):\n",
    "                context = sentence[i:i+self.N-1]\n",
    "                context = \" \".join(context)\n",
    "                word = sentence[i+self.N-1]\n",
    "                counts[context][word] += 1\n",
    "\n",
    "        self.counts = counts\n",
    "        return counts\n",
    "        \n",
    "    # normalize counts into probability(considering smoothing)\n",
    "    def get_prob(self, counter):\n",
    "        total = float(sum(counter.values()))\n",
    "        k = self.smoothing_k\n",
    "        \n",
    "        prob = {}\n",
    "        for word, count in counter.items():\n",
    "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
    "        return prob\n",
    "        \n",
    "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
    "        if self.N == 1 and word in self.prob.keys():\n",
    "            return math.log(self.prob[word]) / seq_len\n",
    "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
    "            return math.log(self.prob[context][word]) / seq_len\n",
    "        else:\n",
    "            # assign a small probability to the unseen ngram\n",
    "            # to avoid log by zero and penalise unseen word or context\n",
    "            return math.log(1/len(self.vocab)) / seq_len\n",
    "        \n",
    "    def get_ngram_prob(self, word, context=\"\"):\n",
    "        if self.N == 1 and word in self.prob.keys():\n",
    "            return self.prob[word]\n",
    "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
    "            return self.prob[context][word]\n",
    "        elif word in self.vocab and self.smoothing_k > 0:\n",
    "            # probability assigned by smoothing\n",
    "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
    "        else:\n",
    "            # unseen word or context\n",
    "            return 0\n",
    "            \n",
    "    # In this method, the perplexity is measured on sentence-level, averaging over all sentences.\n",
    "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
    "    def perplexity(self, test_data):\n",
    "        log_ppl = 0\n",
    "        if self.N == 1:\n",
    "            for sentence in test_data:\n",
    "                for word in sentence:\n",
    "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
    "        else:\n",
    "            for sentence in test_data:\n",
    "                for i in range(len(sentence)-self.N+1):\n",
    "                    context = sentence[i:i+self.N-1]\n",
    "                    context = \" \".join(context)\n",
    "                    word = sentence[i+self.N-1]\n",
    "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
    "                        \n",
    "        log_ppl /= len(test_data)\n",
    "        ppl = math.exp(-log_ppl)\n",
    "        return ppl\n",
    "    \n",
    "    def _is_unseen_ngram(self, context, word):\n",
    "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # generate the most probable k words\n",
    "    def generate_next(self, context, k):\n",
    "        contex = (self.N-1) * '<s>' + context\n",
    "        context = context.split()\n",
    "        ngram_context_list = context[-self.N+1:]\n",
    "        ngram_context = \" \".join(ngram_context_list)\n",
    "        \n",
    "        if ngram_context in self.prob.keys():\n",
    "            candidates = self.prob[ngram_context]\n",
    "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
    "            for i in range(min(k, len(most_probable_words))):\n",
    "                print(\" \".join(context)+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
    "        else:\n",
    "            print(\"Unseen context!\")\n",
    "            \n",
    "    # generate the next n words with greedy search\n",
    "    def generate_next_n(self, context, n):\n",
    "        contex = (self.N-1) * '<s>' + context\n",
    "        context = context.split()\n",
    "        ngram_context_list = context[-self.N+1:]\n",
    "        ngram_context = \" \".join(ngram_context_list)\n",
    "        \n",
    "        for i in range(n):\n",
    "            try:\n",
    "                candidates = self.prob[ngram_context]\n",
    "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
    "                context += [most_likely_next]\n",
    "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
    "                ngram_context = \" \".join(ngram_context_list)\n",
    "            except:\n",
    "                break\n",
    "        print(\" \".join(context))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with toy dataset\n",
    "\n",
    "At this step, let's train a Bigram language model on the toy dataset. The ngram probabilities are shown to give you an intuition of how the ngrams are counted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'like', 'ice', 'cream', '</s>'], ['I', 'like', 'chocolate', '</s>'], ['I', 'hate', 'beans', '</s>']]\n",
      "{'</s>', 'beans', 'chocolate', 'like', 'cream', 'I', 'hate', 'ice'}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"I like ice cream\",\n",
    "         \"I like chocolate\",\n",
    "         \"I hate beans\"]\n",
    "data = [l.strip().split() + ['</s>'] for l in corpus if l.strip()]\n",
    "vocab = set(flatten(data))\n",
    "print(data)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probability(lm):\n",
    "    for context in lm.vocab:\n",
    "        for word in lm.vocab:\n",
    "            prob = lm.get_ngram_prob(word, context)\n",
    "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing\n",
    "Smoothing method is used to deal with the sparsity problem in NGram LM.\n",
    "The probability of frequent words are stolen by the less frequent words.\n",
    "\n",
    "For example, with an add-1 smoothing, the probability is calculated as:\n",
    "\n",
    "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
    "\n",
    "Q1: What is the disadvantage of smoothing?\n",
    "\n",
    "A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(</s>\t|</s>) = 0\n",
      "P(beans\t|</s>) = 0\n",
      "P(<s>\t|</s>) = 0\n",
      "P(chocolate\t|</s>) = 0\n",
      "P(like\t|</s>) = 0\n",
      "P(cream\t|</s>) = 0\n",
      "P(I\t|</s>) = 0\n",
      "P(hate\t|</s>) = 0\n",
      "P(ice\t|</s>) = 0\n",
      "--------------------------\n",
      "P(</s>\t|beans) = 1.0\n",
      "P(beans\t|beans) = 0\n",
      "P(<s>\t|beans) = 0\n",
      "P(chocolate\t|beans) = 0\n",
      "P(like\t|beans) = 0\n",
      "P(cream\t|beans) = 0\n",
      "P(I\t|beans) = 0\n",
      "P(hate\t|beans) = 0\n",
      "P(ice\t|beans) = 0\n",
      "--------------------------\n",
      "P(</s>\t|<s>) = 0\n",
      "P(beans\t|<s>) = 0\n",
      "P(<s>\t|<s>) = 0\n",
      "P(chocolate\t|<s>) = 0\n",
      "P(like\t|<s>) = 0\n",
      "P(cream\t|<s>) = 0\n",
      "P(I\t|<s>) = 1.0\n",
      "P(hate\t|<s>) = 0\n",
      "P(ice\t|<s>) = 0\n",
      "--------------------------\n",
      "P(</s>\t|chocolate) = 1.0\n",
      "P(beans\t|chocolate) = 0\n",
      "P(<s>\t|chocolate) = 0\n",
      "P(chocolate\t|chocolate) = 0\n",
      "P(like\t|chocolate) = 0\n",
      "P(cream\t|chocolate) = 0\n",
      "P(I\t|chocolate) = 0\n",
      "P(hate\t|chocolate) = 0\n",
      "P(ice\t|chocolate) = 0\n",
      "--------------------------\n",
      "P(</s>\t|like) = 0\n",
      "P(beans\t|like) = 0\n",
      "P(<s>\t|like) = 0\n",
      "P(chocolate\t|like) = 0.5\n",
      "P(like\t|like) = 0\n",
      "P(cream\t|like) = 0\n",
      "P(I\t|like) = 0\n",
      "P(hate\t|like) = 0\n",
      "P(ice\t|like) = 0.5\n",
      "--------------------------\n",
      "P(</s>\t|cream) = 1.0\n",
      "P(beans\t|cream) = 0\n",
      "P(<s>\t|cream) = 0\n",
      "P(chocolate\t|cream) = 0\n",
      "P(like\t|cream) = 0\n",
      "P(cream\t|cream) = 0\n",
      "P(I\t|cream) = 0\n",
      "P(hate\t|cream) = 0\n",
      "P(ice\t|cream) = 0\n",
      "--------------------------\n",
      "P(</s>\t|I) = 0\n",
      "P(beans\t|I) = 0\n",
      "P(<s>\t|I) = 0\n",
      "P(chocolate\t|I) = 0\n",
      "P(like\t|I) = 0.6666666666666666\n",
      "P(cream\t|I) = 0\n",
      "P(I\t|I) = 0\n",
      "P(hate\t|I) = 0.3333333333333333\n",
      "P(ice\t|I) = 0\n",
      "--------------------------\n",
      "P(</s>\t|hate) = 0\n",
      "P(beans\t|hate) = 1.0\n",
      "P(<s>\t|hate) = 0\n",
      "P(chocolate\t|hate) = 0\n",
      "P(like\t|hate) = 0\n",
      "P(cream\t|hate) = 0\n",
      "P(I\t|hate) = 0\n",
      "P(hate\t|hate) = 0\n",
      "P(ice\t|hate) = 0\n",
      "--------------------------\n",
      "P(</s>\t|ice) = 0\n",
      "P(beans\t|ice) = 0\n",
      "P(<s>\t|ice) = 0\n",
      "P(chocolate\t|ice) = 0\n",
      "P(like\t|ice) = 0\n",
      "P(cream\t|ice) = 1.0\n",
      "P(I\t|ice) = 0\n",
      "P(hate\t|ice) = 0\n",
      "P(ice\t|ice) = 0\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "lm = NGramLM(2)\n",
    "lm.train(vocab, data, smoothing_k=0)\n",
    "\n",
    "######################################################\n",
    "# Q2: try with add-1 smoothing and see the probability\n",
    "######################################################\n",
    "# lm.train(vocab, data, smoothing_k=1)\n",
    "\n",
    "print_probability(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on WikiText-2 dataset and evaluate perplexity\n",
    "### Evaluating perplexity\n",
    "\n",
    "Q3: why do we need to calculate log perplexity?\n",
    "\n",
    "A:\n",
    "\n",
    "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
    "\n",
    "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33278\n"
     ]
    }
   ],
   "source": [
    "vocab, train_data = prepare_data('train.txt')\n",
    "_, valid_data = prepare_data('valid.txt')\n",
    "_, test_data = prepare_data('test.txt')\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = NGramLM(3)\n",
    "lm.train(vocab, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449.27961912112\n",
      "355.09754856529463\n"
     ]
    }
   ],
   "source": [
    "print(lm.perplexity(valid_data))\n",
    "print(lm.perplexity(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating sentence\n",
    "With the pre-trained NGram language model, we can predict possible next words with given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eggs hatch at night , and the larvae swim to the water surface where they drift with the ocean currents , preying on <unk> . This stage involves three <unk> and lasts for 15 â€“ 35 days . After the third moult , the juvenile takes on a form closer to the adult , and adopts a <unk> lifestyle . The juveniles are rarely seen in the wild , and are poorly known , although they are known to be capable of digging extensive burrows . It is estimated that only 1 larva in every 20 @,@ 000 survives to the <unk> phase . When they reach a carapace length of 15 mm ( 0 @.@ 59 in ) , the juveniles leave their burrows and start their adult lives . </s>\n",
      "The eggs hatch at night , and the larvae swim to the water surface where they were\t P=0.12408759124087591\n",
      "The eggs hatch at night , and the larvae swim to the water surface where they are\t P=0.08029197080291971\n",
      "The eggs hatch at night , and the larvae swim to the water surface where they would\t P=0.051094890510948905\n"
     ]
    }
   ],
   "source": [
    "# generate the most probable following words given the context\n",
    "print(\" \".join(valid_data[12]))\n",
    "\n",
    "# actually the only useful contexts in the Trigram LM is [\"where\", \"they\"]\n",
    "context = \"The eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
    "\n",
    "lm.generate_next(context, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk>\n",
      "The eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk> of the <unk> of the <unk> of the <unk> of\n"
     ]
    }
   ],
   "source": [
    "# generate the next n words with greedy search\n",
    "lm.generate_next_n(context, 10)\n",
    "\n",
    "# This is not a good method in practice,\n",
    "# because wrong predictions in the early steps would hinder the following predictions\n",
    "lm.generate_next_n(context, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of N\n",
    "\n",
    "Q4: Why does the perplexity increase when N is large?\n",
    "\n",
    "A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "1-gram LM perplexity on valid set: 727.3800286163042\n",
      "1-gram LM perplexity on test  set: 661.8109371098135\n",
      "************************\n",
      "2-gram LM perplexity on valid set: 141.41008166795802\n",
      "2-gram LM perplexity on test  set: 121.75625147213104\n",
      "************************\n",
      "3-gram LM perplexity on valid set: 449.27961912112\n",
      "3-gram LM perplexity on test  set: 355.09754856529463\n",
      "************************\n",
      "4-gram LM perplexity on valid set: 1251.3004060217763\n",
      "4-gram LM perplexity on test  set: 968.4259408279687\n",
      "************************\n",
      "5-gram LM perplexity on valid set: 1754.5532742330947\n",
      "5-gram LM perplexity on test  set: 1344.5022350288755\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,6):\n",
    "    lm = NGramLM(n)\n",
    "    lm.train(vocab, train_data)\n",
    "    print(\"************************\")\n",
    "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
    "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation\n",
    "Adding interpolation would utilize different ngram context, which would alleviate the problem when N grows large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolateNGramLM(NGramLM):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        super(InterpolateNGramLM, self).__init__(N)\n",
    "        self.ngram_lms = []\n",
    "        self.lambdas = []\n",
    "        \n",
    "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
    "        assert len(lambdas) == self.N\n",
    "        assert sum(lambdas) - 1 < 1e-9\n",
    "        self.vocab = vocab\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        for i in range(self.N, 0, -1):\n",
    "            lm = NGramLM(i)\n",
    "            print(\"Training {}-gram language model\".format(i))\n",
    "            lm.train(vocab, data, smoothing_k)\n",
    "            self.ngram_lms.append(lm)\n",
    "    \n",
    "    def get_ngram_logprob(self, word, seq_len, context):\n",
    "        prob = 0\n",
    "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
    "            context_words = context.split()\n",
    "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
    "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
    "        return math.log(prob) / seq_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3-gram language model\n",
      "Training 2-gram language model\n",
      "Training 1-gram language model\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Q5: tune your coefficients to decrease perplexity\n",
    "##################################################\n",
    "ilm = InterpolateNGramLM(3)\n",
    "ilm.train(vocab, train_data, lambdas=[\"coefficients here\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140.07040126740415\n",
      "111.56423076848951\n"
     ]
    }
   ],
   "source": [
    "print(ilm.perplexity(valid_data))\n",
    "print(ilm.perplexity(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
