{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download WikiText-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:18.265129Z",
     "start_time": "2020-01-29T18:30:17.460952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-29 18:30:17--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.56.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.56.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10797148 (10M) [text/plain]\n",
      "Saving to: ‘train.txt’\n",
      "\n",
      "train.txt           100%[===================>]  10.30M  30.9MB/s    in 0.3s    \n",
      "\n",
      "2020-01-29 18:30:17 (30.9 MB/s) - ‘train.txt’ saved [10797148/10797148]\n",
      "\n",
      "--2020-01-29 18:30:17--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.56.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.56.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1121681 (1.1M) [text/plain]\n",
      "Saving to: ‘valid.txt’\n",
      "\n",
      "valid.txt           100%[===================>]   1.07M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-01-29 18:30:17 (24.1 MB/s) - ‘valid.txt’ saved [1121681/1121681]\n",
      "\n",
      "--2020-01-29 18:30:18--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.56.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.56.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1256449 (1.2M) [text/plain]\n",
      "Saving to: ‘test.txt’\n",
      "\n",
      "test.txt            100%[===================>]   1.20M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-01-29 18:30:18 (27.8 MB/s) - ‘test.txt’ saved [1256449/1256449]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
    "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:18.271133Z",
     "start_time": "2020-01-29T18:30:18.266713Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import operator\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# some helper functions\n",
    "def prepare_data(filename):\n",
    "    data = [l.strip().lower().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
    "    corpus = flatten(data)\n",
    "    vocab = set(corpus)\n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram LM\n",
    "A language model assign a probability to each possible next word given a history of previous words (context). \n",
    "\n",
    "$P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$\n",
    "\n",
    "### Markov Assumption\n",
    "Since calculating the probability of the whole sentence is not feasible, the Markov Assumption is introduced.\n",
    "\n",
    "It assumes that each next word only depends on the previous K words (In an N-Gram language model, K = N-1).\n",
    "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
    "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
    "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
    "\n",
    "For an N-Gram language model, the probability is calculated by counting the frequency:\n",
    "\n",
    "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:18.290242Z",
     "start_time": "2020-01-29T18:30:18.272506Z"
    }
   },
   "outputs": [],
   "source": [
    "class NGramLM():\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.vocab = set()\n",
    "        self.data = []\n",
    "        self.prob = {}\n",
    "        self.count = defaultdict(Counter)\n",
    "    \n",
    "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
    "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
    "    def train(self, vocab, data, smoothing_k=0):\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.smoothing_k = smoothing_k\n",
    "\n",
    "        if self.N == 1:\n",
    "            self.counts = Counter(flatten(data))\n",
    "            self.prob = self.get_prob(self.counts)\n",
    "        else:\n",
    "            self.vocab.add('<s>')\n",
    "            counts = self.count_ngram()\n",
    "            \n",
    "            self.prob = {}\n",
    "            for context, counter in counts.items():\n",
    "                self.prob[context] = self.get_prob(counter)\n",
    "    \n",
    "    def count_ngram(self):\n",
    "        counts = defaultdict(Counter)\n",
    "        for sentence in self.data:\n",
    "            sentence = (self.N - 1) * ['<s>'] + sentence \n",
    "            for i in range(len(sentence)-self.N+1):\n",
    "                context = sentence[i:i+self.N-1]\n",
    "                context = \" \".join(context)\n",
    "                word = sentence[i+self.N-1]\n",
    "                counts[context][word] += 1\n",
    "\n",
    "        self.counts = counts\n",
    "        return counts\n",
    "        \n",
    "    # normalize counts into probability(considering smoothing)\n",
    "    def get_prob(self, counter):\n",
    "        total = float(sum(counter.values()))\n",
    "        k = self.smoothing_k\n",
    "        \n",
    "        prob = {}\n",
    "        for word, count in counter.items():\n",
    "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
    "        return prob\n",
    "        \n",
    "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
    "        if self.N == 1 and word in self.prob.keys():\n",
    "            return math.log(self.prob[word]) / seq_len\n",
    "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
    "            return math.log(self.prob[context][word]) / seq_len\n",
    "        else:\n",
    "            # assign a small probability to the unseen ngram\n",
    "            # to avoid log of zero and to penalise unseen word or context\n",
    "            return math.log(1/len(self.vocab)) / seq_len\n",
    "        \n",
    "    def get_ngram_prob(self, word, context=\"\"):\n",
    "        if self.N == 1 and word in self.prob.keys():\n",
    "            return self.prob[word]\n",
    "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
    "            return self.prob[context][word]\n",
    "        elif word in self.vocab and self.smoothing_k > 0:\n",
    "            # probability assigned by smoothing\n",
    "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
    "        else:\n",
    "            # unseen word or context\n",
    "            return 0\n",
    "            \n",
    "    # In this method, the perplexity is measured at the sentence-level, averaging over all sentences.\n",
    "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
    "    def perplexity(self, test_data):\n",
    "        log_ppl = 0\n",
    "        if self.N == 1:\n",
    "            for sentence in test_data:\n",
    "                for word in sentence:\n",
    "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
    "        else:\n",
    "            for sentence in test_data:\n",
    "                for i in range(len(sentence)-self.N+1):\n",
    "                    context = sentence[i:i+self.N-1]\n",
    "                    context = \" \".join(context)\n",
    "                    word = sentence[i+self.N-1]\n",
    "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
    "                        \n",
    "        log_ppl /= len(test_data)\n",
    "        ppl = math.exp(-log_ppl)\n",
    "        return ppl\n",
    "    \n",
    "    def _is_unseen_ngram(self, context, word):\n",
    "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # generate the most probable k words\n",
    "    def generate_next(self, context, k):\n",
    "        contex = (self.N-1) * '<s>' + context\n",
    "        context = context.split()\n",
    "        ngram_context_list = context[-self.N+1:]\n",
    "        ngram_context = \" \".join(ngram_context_list)\n",
    "        \n",
    "        if ngram_context in self.prob.keys():\n",
    "            candidates = self.prob[ngram_context]\n",
    "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
    "            for i in range(min(k, len(most_probable_words))):\n",
    "                print(\" \".join(context)+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
    "        else:\n",
    "            print(\"Unseen context!\")\n",
    "            \n",
    "    # generate the next n words with greedy search\n",
    "    def generate_next_n(self, context, n):\n",
    "        contex = (self.N-1) * '<s>' + context\n",
    "        context = context.split()\n",
    "        ngram_context_list = context[-self.N+1:]\n",
    "        ngram_context = \" \".join(ngram_context_list)\n",
    "        \n",
    "        for i in range(n):\n",
    "            try:\n",
    "                candidates = self.prob[ngram_context]\n",
    "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
    "                context += [most_likely_next]\n",
    "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
    "                ngram_context = \" \".join(ngram_context_list)\n",
    "            except:\n",
    "                break\n",
    "        print(\" \".join(context))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with toy dataset\n",
    "\n",
    "At this step, let's train a Bigram language model on the toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:18.297355Z",
     "start_time": "2020-01-29T18:30:18.291387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like', 'ice', 'cream', '</s>'], ['i', 'like', 'chocolate', '</s>'], ['i', 'hate', 'beans', '</s>']]\n",
      "{'like', 'beans', 'cream', 'ice', '</s>', 'chocolate', 'hate', 'i'}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"I like ice cream\",\n",
    "         \"I like chocolate\",\n",
    "         \"I hate beans\"]\n",
    "data = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
    "vocab = set(flatten(data))\n",
    "print(data)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:18.301895Z",
     "start_time": "2020-01-29T18:30:18.298317Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_probability(lm):\n",
    "    for context in lm.vocab:\n",
    "        for word in lm.vocab:\n",
    "            prob = lm.get_ngram_prob(word, context)\n",
    "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing\n",
    "Smoothing method is used to deal with the sparsity problem in the N-Gram LM.\n",
    "The probability mass is shifted towards the less frequent words.\n",
    "\n",
    "For example, with an add-1 smoothing, the probability is calculated as:\n",
    "\n",
    "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
    "\n",
    "Q1: What is the disadvantage of smoothing?\n",
    "\n",
    "A: If there are too many word with zero counts, the frequent words will sacrifice more probability, which might lead to higher perplexity on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:18.310269Z",
     "start_time": "2020-01-29T18:30:18.302761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(like\t|like) = 0\n",
      "P(beans\t|like) = 0\n",
      "P(cream\t|like) = 0\n",
      "P(ice\t|like) = 0.5\n",
      "P(</s>\t|like) = 0\n",
      "P(<s>\t|like) = 0\n",
      "P(chocolate\t|like) = 0.5\n",
      "P(hate\t|like) = 0\n",
      "P(i\t|like) = 0\n",
      "--------------------------\n",
      "P(like\t|beans) = 0\n",
      "P(beans\t|beans) = 0\n",
      "P(cream\t|beans) = 0\n",
      "P(ice\t|beans) = 0\n",
      "P(</s>\t|beans) = 1.0\n",
      "P(<s>\t|beans) = 0\n",
      "P(chocolate\t|beans) = 0\n",
      "P(hate\t|beans) = 0\n",
      "P(i\t|beans) = 0\n",
      "--------------------------\n",
      "P(like\t|cream) = 0\n",
      "P(beans\t|cream) = 0\n",
      "P(cream\t|cream) = 0\n",
      "P(ice\t|cream) = 0\n",
      "P(</s>\t|cream) = 1.0\n",
      "P(<s>\t|cream) = 0\n",
      "P(chocolate\t|cream) = 0\n",
      "P(hate\t|cream) = 0\n",
      "P(i\t|cream) = 0\n",
      "--------------------------\n",
      "P(like\t|ice) = 0\n",
      "P(beans\t|ice) = 0\n",
      "P(cream\t|ice) = 1.0\n",
      "P(ice\t|ice) = 0\n",
      "P(</s>\t|ice) = 0\n",
      "P(<s>\t|ice) = 0\n",
      "P(chocolate\t|ice) = 0\n",
      "P(hate\t|ice) = 0\n",
      "P(i\t|ice) = 0\n",
      "--------------------------\n",
      "P(like\t|</s>) = 0\n",
      "P(beans\t|</s>) = 0\n",
      "P(cream\t|</s>) = 0\n",
      "P(ice\t|</s>) = 0\n",
      "P(</s>\t|</s>) = 0\n",
      "P(<s>\t|</s>) = 0\n",
      "P(chocolate\t|</s>) = 0\n",
      "P(hate\t|</s>) = 0\n",
      "P(i\t|</s>) = 0\n",
      "--------------------------\n",
      "P(like\t|<s>) = 0\n",
      "P(beans\t|<s>) = 0\n",
      "P(cream\t|<s>) = 0\n",
      "P(ice\t|<s>) = 0\n",
      "P(</s>\t|<s>) = 0\n",
      "P(<s>\t|<s>) = 0\n",
      "P(chocolate\t|<s>) = 0\n",
      "P(hate\t|<s>) = 0\n",
      "P(i\t|<s>) = 1.0\n",
      "--------------------------\n",
      "P(like\t|chocolate) = 0\n",
      "P(beans\t|chocolate) = 0\n",
      "P(cream\t|chocolate) = 0\n",
      "P(ice\t|chocolate) = 0\n",
      "P(</s>\t|chocolate) = 1.0\n",
      "P(<s>\t|chocolate) = 0\n",
      "P(chocolate\t|chocolate) = 0\n",
      "P(hate\t|chocolate) = 0\n",
      "P(i\t|chocolate) = 0\n",
      "--------------------------\n",
      "P(like\t|hate) = 0\n",
      "P(beans\t|hate) = 1.0\n",
      "P(cream\t|hate) = 0\n",
      "P(ice\t|hate) = 0\n",
      "P(</s>\t|hate) = 0\n",
      "P(<s>\t|hate) = 0\n",
      "P(chocolate\t|hate) = 0\n",
      "P(hate\t|hate) = 0\n",
      "P(i\t|hate) = 0\n",
      "--------------------------\n",
      "P(like\t|i) = 0.6666666666666666\n",
      "P(beans\t|i) = 0\n",
      "P(cream\t|i) = 0\n",
      "P(ice\t|i) = 0\n",
      "P(</s>\t|i) = 0\n",
      "P(<s>\t|i) = 0\n",
      "P(chocolate\t|i) = 0\n",
      "P(hate\t|i) = 0.3333333333333333\n",
      "P(i\t|i) = 0\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "lm = NGramLM(2)\n",
    "lm.train(vocab, data, smoothing_k=0)\n",
    "\n",
    "######################################################\n",
    "# Q2: try with add-1 smoothing and see the probability\n",
    "######################################################\n",
    "# lm.train(vocab, data, smoothing_k=1)\n",
    "\n",
    "print_probability(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on WikiText-2 dataset and evaluate perplexity\n",
    "### Evaluating perplexity\n",
    "\n",
    "Q3: why do we need to calculate log perplexity?\n",
    "\n",
    "A: The chain rule in the Markov Assumption would lead to a continuous multiplication of many small probabilities, which might lead to underflow. We use log perplexity to replace multiplication into addition.\n",
    "\n",
    "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
    "\n",
    "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:18.712903Z",
     "start_time": "2020-01-29T18:30:18.312677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28912\n"
     ]
    }
   ],
   "source": [
    "vocab, train_data = prepare_data('train.txt')\n",
    "_, valid_data = prepare_data('valid.txt')\n",
    "_, test_data = prepare_data('test.txt')\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:22.224402Z",
     "start_time": "2020-01-29T18:30:18.714081Z"
    }
   },
   "outputs": [],
   "source": [
    "lm = NGramLM(3)\n",
    "lm.train(vocab, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:22.811004Z",
     "start_time": "2020-01-29T18:30:22.225566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387.1190374834364\n",
      "310.6880368161233\n"
     ]
    }
   ],
   "source": [
    "print(lm.perplexity(valid_data))\n",
    "print(lm.perplexity(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating sentence\n",
    "With the pre-trained N-Gram language model, we can predict possible next words with given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:22.815241Z",
     "start_time": "2020-01-29T18:30:22.812141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the eggs hatch at night , and the larvae swim to the water surface where they drift with the ocean currents , preying on <unk> . this stage involves three <unk> and lasts for 15 – 35 days . after the third moult , the juvenile takes on a form closer to the adult , and adopts a <unk> lifestyle . the juveniles are rarely seen in the wild , and are poorly known , although they are known to be capable of digging extensive burrows . it is estimated that only 1 larva in every 20 @,@ 000 survives to the <unk> phase . when they reach a carapace length of 15 mm ( 0 @.@ 59 in ) , the juveniles leave their burrows and start their adult lives . </s>\n",
      "The eggs hatch at night , and the larvae swim to the water surface where they were\t P=0.12408759124087591\n",
      "The eggs hatch at night , and the larvae swim to the water surface where they are\t P=0.08029197080291971\n",
      "The eggs hatch at night , and the larvae swim to the water surface where they would\t P=0.051094890510948905\n"
     ]
    }
   ],
   "source": [
    "# generate the most probable following words given the context\n",
    "print(\" \".join(valid_data[12]))\n",
    "\n",
    "# actually the only useful contexts in the Trigram LM is [\"where\", \"they\"]\n",
    "context = \"The eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
    "\n",
    "lm.generate_next(context, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:22.828378Z",
     "start_time": "2020-01-29T18:30:22.816839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk>\n",
      "The eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk> of the <unk> of the <unk> of the <unk> of\n"
     ]
    }
   ],
   "source": [
    "# generate the next n words with greedy search\n",
    "lm.generate_next_n(context, 10)\n",
    "\n",
    "# This is not a good method in practice,\n",
    "# because wrong predictions in the early steps would introduce errors to the following predictions\n",
    "lm.generate_next_n(context, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of N\n",
    "\n",
    "Q4: Why does the perplexity increase when N is large?\n",
    "\n",
    "A: Because with larger N, the context (previous N-1 words) in the valid/test data is not likely to be seen in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:41.930826Z",
     "start_time": "2020-01-29T18:30:22.829892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "1-gram LM perplexity on valid set: 599.6450225274759\n",
      "1-gram LM perplexity on test  set: 553.0467265986567\n",
      "************************\n",
      "2-gram LM perplexity on valid set: 130.75949726991627\n",
      "2-gram LM perplexity on test  set: 114.76592406312065\n",
      "************************\n",
      "3-gram LM perplexity on valid set: 387.1190374834364\n",
      "3-gram LM perplexity on test  set: 310.6880368161233\n",
      "************************\n",
      "4-gram LM perplexity on valid set: 1091.9348578736633\n",
      "4-gram LM perplexity on test  set: 846.7563102382584\n",
      "************************\n",
      "5-gram LM perplexity on valid set: 1558.823545905558\n",
      "5-gram LM perplexity on test  set: 1201.746345143052\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,6):\n",
    "    lm = NGramLM(n)\n",
    "    lm.train(vocab, train_data)\n",
    "    print(\"************************\")\n",
    "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
    "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation\n",
    "In interpolation, we mix the probability estimates from all the n-gram estimators to alleviate the sparsity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:41.937643Z",
     "start_time": "2020-01-29T18:30:41.932047Z"
    }
   },
   "outputs": [],
   "source": [
    "class InterpolateNGramLM(NGramLM):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        super(InterpolateNGramLM, self).__init__(N)\n",
    "        self.ngram_lms = []\n",
    "        self.lambdas = []\n",
    "        \n",
    "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
    "        assert len(lambdas) == self.N\n",
    "        assert sum(lambdas) - 1 < 1e-9\n",
    "        self.vocab = vocab\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        for i in range(self.N, 0, -1):\n",
    "            lm = NGramLM(i)\n",
    "            print(\"Training {}-gram language model\".format(i))\n",
    "            lm.train(vocab, data, smoothing_k)\n",
    "            self.ngram_lms.append(lm)\n",
    "    \n",
    "    def get_ngram_logprob(self, word, seq_len, context):\n",
    "        prob = 0\n",
    "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
    "            context_words = context.split()\n",
    "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
    "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
    "        return math.log(prob) / seq_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:46.507705Z",
     "start_time": "2020-01-29T18:30:41.939076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3-gram language model\n",
      "Training 2-gram language model\n",
      "Training 1-gram language model\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# Q5: tune your coefficients to decrease perplexity\n",
    "###################################################\n",
    "ilm = InterpolateNGramLM(3)\n",
    "ilm.train(vocab, train_data, lambdas=[0.5, 0.4, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:30:48.376911Z",
     "start_time": "2020-01-29T18:30:46.509150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.26886448793077\n",
      "103.0773276220259\n"
     ]
    }
   ],
   "source": [
    "print(ilm.perplexity(valid_data))\n",
    "print(ilm.perplexity(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
