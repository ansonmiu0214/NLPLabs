{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_preprocessing_and_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5qS8y5_Ewv6n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "52e1ac70-7850-42ff-bc80-bdf8dd59ed61"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FE8B9-L8U0aZ"
      },
      "source": [
        "#Semantic word representations\n",
        "\n",
        "\n",
        "Recall that *semantic Word Representations* are representations that are learned to capture the 'meaning' of a word. These are low-dimensional vectors that contain some semantic properties. In this notebook we are going to build state-of-the art approaches to obtain semantic word representations using the **word2vec** modelling approach. We will also use these vectors in some  tasks to understand the utility of these representations. \n",
        "\n",
        "We begin by loading some of the libraries that are necessary for building our model. We are using [pytorch](https://pytorch.org/), an open source deep learning platform, as our backbone library in the course. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XG82efAzvcbK",
        "colab": {}
      },
      "source": [
        "#@title Loading packages\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from tqdm import tqdm \n",
        "import codecs\n",
        "from sklearn.metrics.pairwise import cosine_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "abMSnMwJx8bu",
        "colab": {}
      },
      "source": [
        "#@title Sample corpora\n",
        "\n",
        "corpus = [\n",
        "    'he is a king',\n",
        "    'she is a queen',\n",
        "    'he is a Man',\n",
        "    'she Is a woman',\n",
        "    'london is, the capital of England',\n",
        "    'Berlin is ... the capital of germany',\n",
        "    'paris is the capital of france.',\n",
        "    'He will eat cake, pie, and/or brownies',\n",
        "    \"she didn't like the brownies\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0hPlpqVFYpfL"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "Q: What is a token and why do we need to tokenize? \n",
        "\n",
        "> A token is a contiguous sequence of characters between two spaces, or between a space and punctuation.\n",
        "> We need to tokenise to identify boundaries between words, i.e. word segmentation.\n",
        "\n",
        "Q: Print the tokenized corpus above. What mistakes do you find in the code below? \n",
        "\n",
        "> `for token in sentence.split(' ')` preserves the capitalisation.\n",
        "\n",
        "Q: What could be a nice way of fixing these mistakes? \n",
        "\n",
        "> Case normalisation.\n",
        "\n",
        "##### 10 mins "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UQ5IL1e1GVn6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "acffd382-3451-4c40-c703-adf7bbdecc15"
      },
      "source": [
        "tokenized_corpus = [[token.lower() for token in sentence.split(' ')]\n",
        "                    for sentence in corpus]\n",
        "print(tokenized_corpus)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['london', 'is,', 'the', 'capital', 'of', 'england'], ['berlin', 'is', '...', 'the', 'capital', 'of', 'germany'], ['paris', 'is', 'the', 'capital', 'of', 'france.'], ['he', 'will', 'eat', 'cake,', 'pie,', 'and/or', 'brownies'], ['she', \"didn't\", 'like', 'the', 'brownies']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NPWECLTEmAG9"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "Tokenization is a crucial pre-processing step in the NLP domain`*`. However, other pre-processing techniques also exist, many of which were extensively employed in rule-based and statistical NLP. While we don't utilise these pre-processing techniques in neural-based NLP anymore, they are still worth a recap. Typically, **stop words** and **punctuation** removal are employed, along with *either* **stemming** or **lemmatization**. However, in the following code, we will demonstrate each of the techniques separately (mainly due to our corpus being so small)\n",
        "\n",
        "### Stop Word removal\n",
        "Stop words are generally the most common words in the language which who's meaning in a sequenece is ambiguous. Some examples of stop words are: The, a, an, that.\n",
        "\n",
        "### Punctuation removal\n",
        "Old school NLP techniques (and some modern day ones) struggle to understand the semantics of punctuation. Thus, they were also removed.\n",
        "\n",
        "## Stemming and Lemmatization\n",
        "Stemming and Lemmatization are two distinct word normalization techniques. Essentially this means that, given our corpora, we wish to have variants of a word in a 'normal' form. For example, [playing, plays, played] may be normalised to \"Play\". The sentence \"the boy's cars are different colours\" may be normalised to \"the boy car be differ colour\"\n",
        "\n",
        "### Stemming\n",
        "In the case of stemming, we want to normalise all words to their stem (or root). The stem is the part of the word to which affixes (suffixes or prefixes) are assigned. Stemming a word may result in the word not actually being a word. For example, some stemming algorithms may stem [trouble, troubling, troubled] as \"troubl\".\n",
        "\n",
        "### Lemmatization\n",
        "Lemmatization attempts to properly reduce unnormalized tokens to a word that belongs in the language. The root word is called a **lemma**, and is the canonical form of a set of words. For example, [runs, running, ran] are all forms of the word \"run.\n",
        "\n",
        "\n",
        "\n",
        "Q. Think of two or three other stop words, and add them to the list of stop words below.\n",
        "\n",
        "> `\"is\", \"and\", \"or\", \"of\"`\n",
        "\n",
        "Q. Write some code which both removes stop words and punctuation from our corpus\n",
        "\n",
        "Q. The examples of stemming and lemmatization below are on words/sequences not in our corpus. Extend the code so it works on our corpus.\n",
        "\n",
        "##### 10 mins \n",
        "\n",
        "N.B. We are not going to use these techniques in this file after this section, so we will demonstrate how to perform these techniques distinctly on our toy corpus.\n",
        "\n",
        "`*`Recently there has been newer approaches to \"tokenization\" which goes further than one token being one word. One example is [SentencePiece](https://github.com/google/sentencepiece). These approaches are out of scope for this lab session, but may appear in future sessions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k_DGoHbPmAG-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3e61983b-f279-4db4-edcd-0ad5a770d349"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # Download the tokenizer model\n",
        "nltk.download('wordnet') # Download the wordnet corpora"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FuSjoUpfmAHA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9d0fb44a-2ebb-403f-f7ba-37b248164053"
      },
      "source": [
        "# STOP WORD REMOVAL\n",
        "stop_words_list = [\"the\", \"a\", \"an\", \"that\",\n",
        "                   \"is\", \"and\", \"or\", \"of\"] # Add stop words from Q1 here.\n",
        "\n",
        "# SWR = stop words removed\n",
        "tokenized_corpus_SWR = []\n",
        "for sentence in corpus:\n",
        "    tokenized_sentence_SWR = []\n",
        "    \n",
        "    for token in sentence.split(\" \"):\n",
        "        if token not in stop_words_list:\n",
        "            tokenized_sentence_SWR.append(token)\n",
        "\n",
        "    if tokenized_sentence_SWR: # Only append to corpus if tokenized_sentence_SWR isn't empty\n",
        "        tokenized_corpus_SWR.append(tokenized_sentence_SWR)\n",
        "        \n",
        "print(tokenized_corpus_SWR)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['he', 'king'], ['she', 'queen'], ['he', 'Man'], ['she', 'Is', 'woman'], ['london', 'is,', 'capital', 'England'], ['Berlin', '...', 'capital', 'germany'], ['paris', 'capital', 'france.'], ['He', 'will', 'eat', 'cake,', 'pie,', 'and/or', 'brownies'], ['she', \"didn't\", 'like', 'brownies']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3VtK8AQ_mAHC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "35174cbb-38aa-47a4-a9b8-830639355f78"
      },
      "source": [
        "# PUNCTUATION REMOVAL\n",
        "import re # regex\n",
        "\n",
        "re_punctuation_string = '[\\s,/.\\']'\n",
        "\n",
        "# PR = punctuation removed\n",
        "tokenized_corpus_PR = []\n",
        "for sentence in corpus:\n",
        "    tokenized_sentence_PR = re.split(re_punctuation_string, sentence) # in python's regex, [...] is an alternative to writing .|.|.\n",
        "    tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # remove empty strings from list \n",
        "    tokenized_corpus_PR.append(tokenized_sentence_PR)\n",
        "        \n",
        "print(tokenized_corpus_PR)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'Man'], ['she', 'Is', 'a', 'woman'], ['london', 'is', 'the', 'capital', 'of', 'England'], ['Berlin', 'is', 'the', 'capital', 'of', 'germany'], ['paris', 'is', 'the', 'capital', 'of', 'france'], ['He', 'will', 'eat', 'cake', 'pie', 'and', 'or', 'brownies'], ['she', 'didn', 't', 'like', 'the', 'brownies']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YyzTW_wXmAHF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6c64a233-a113-4d24-bc63-f04cdabdbd15"
      },
      "source": [
        "# ANSWER Q2 HERE\n",
        "stop_words_set = set(stop_words_list)\n",
        "\n",
        "tokenized_corpus_SWR_PR = []\n",
        "for sentence in corpus:\n",
        "    tokenized_sentence_PR = re.split(re_punctuation_string, sentence)\n",
        "    tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR))\n",
        "    tokenized_sentence_PR = [token for token in tokenized_sentence_PR\n",
        "                             if token not in stop_words_set]\n",
        "    tokenized_corpus_SWR_PR.append(tokenized_sentence_PR)\n",
        "\n",
        "\"\"\"\n",
        "Alternatively:\n",
        "[[token for token in re.split(re_punctuation_string, sentence)\n",
        "  if token and token not in stop_words_set]\n",
        " for sentence in corpus]\n",
        "\"\"\"\n",
        "\n",
        "print(tokenized_corpus_SWR_PR)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['he', 'king'], ['she', 'queen'], ['he', 'Man'], ['she', 'Is', 'woman'], ['london', 'capital', 'England'], ['Berlin', 'capital', 'germany'], ['paris', 'capital', 'france'], ['He', 'will', 'eat', 'cake', 'pie', 'brownies'], ['she', 'didn', 't', 'like', 'brownies']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZGX31WamAHH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5d5115a2-c4f2-4c38-d84d-4df29a29af4b"
      },
      "source": [
        "# STEMMING\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "stemming_word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Stemmed variant\"))\n",
        "print()\n",
        "\n",
        "for word in stemming_word_list:\n",
        "      print(\"{0:20}{1:20}\".format(word,porter.stem(word)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Stemmed variant     \n",
            "\n",
            "friend              friend              \n",
            "friendship          friendship          \n",
            "friends             friend              \n",
            "friendships         friendship          \n",
            "stabil              stabil              \n",
            "destabilize         destabil            \n",
            "misunderstanding    misunderstand       \n",
            "railroad            railroad            \n",
            "moonlight           moonlight           \n",
            "football            footbal             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Muvs2GZ4mAHK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "42196b67-6683-4b4f-f140-eb771b57e9d0"
      },
      "source": [
        "# LEMMATIZATION\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "to_lemmatize_sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "\n",
        "# lemmatization requires punctuation removal\n",
        "to_lemmatize_sentence = re.split(re_punctuation_string, to_lemmatize_sentence)\n",
        "to_lemmatize_sentence = list(filter(None, to_lemmatize_sentence))\n",
        "\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "print()\n",
        "\n",
        "for word in to_lemmatize_sentence:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "\n",
            "He                  He                  \n",
            "was                 wa                  \n",
            "running             running             \n",
            "and                 and                 \n",
            "eating              eating              \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 ha                  \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swimming            \n",
            "after               after               \n",
            "playing             playing             \n",
            "long                long                \n",
            "hours               hour                \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CL6qkUJPmAHM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "255fc4e0-7986-466d-fbbb-bc5421f8a8c3"
      },
      "source": [
        "# Why didn't the above do anything?\n",
        "# It's because the lemmatizer requires parts of speech (POS) context about the word it is currently parsing.\n",
        "# We would need to use a POS model to identify what the POS for a token in its context is.\n",
        "# In the above example (and for Q3), we'll just pass in the VERB context for every token\n",
        "\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "print()\n",
        "\n",
        "for word in to_lemmatize_sentence:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "\n",
            "He                  He                  \n",
            "was                 be                  \n",
            "running             run                 \n",
            "and                 and                 \n",
            "eating              eat                 \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 have                \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swim                \n",
            "after               after               \n",
            "playing             play                \n",
            "long                long                \n",
            "hours               hours               \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qXfoaPJ3mAHP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "ed3a3137-74bf-4376-ea58-9c04207cb697"
      },
      "source": [
        "# ANSWER Q3 HERE (make sure not to overwrite our \"tokenized_corpus\" variable from the \"\")\n",
        "\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Stemmed variant\",\"Lemma\"))\n",
        "print()\n",
        "\n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        stemmed = porter.stem(token)\n",
        "        lemma = wordnet_lemmatizer.lemmatize(token, pos='v')\n",
        "        print(\"{0:20}{1:20}{2:20}\".format(token, stemmed, lemma))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Stemmed variant     Lemma               \n",
            "\n",
            "he                  he                  he                  \n",
            "is                  is                  be                  \n",
            "a                   a                   a                   \n",
            "king                king                king                \n",
            "she                 she                 she                 \n",
            "is                  is                  be                  \n",
            "a                   a                   a                   \n",
            "queen               queen               queen               \n",
            "he                  he                  he                  \n",
            "is                  is                  be                  \n",
            "a                   a                   a                   \n",
            "man                 man                 man                 \n",
            "she                 she                 she                 \n",
            "is                  is                  be                  \n",
            "a                   a                   a                   \n",
            "woman               woman               woman               \n",
            "london              london              london              \n",
            "is,                 is,                 is,                 \n",
            "the                 the                 the                 \n",
            "capital             capit               capital             \n",
            "of                  of                  of                  \n",
            "england             england             england             \n",
            "berlin              berlin              berlin              \n",
            "is                  is                  be                  \n",
            "...                 ...                 ...                 \n",
            "the                 the                 the                 \n",
            "capital             capit               capital             \n",
            "of                  of                  of                  \n",
            "germany             germani             germany             \n",
            "paris               pari                paris               \n",
            "is                  is                  be                  \n",
            "the                 the                 the                 \n",
            "capital             capit               capital             \n",
            "of                  of                  of                  \n",
            "france.             france.             france.             \n",
            "he                  he                  he                  \n",
            "will                will                will                \n",
            "eat                 eat                 eat                 \n",
            "cake,               cake,               cake,               \n",
            "pie,                pie,                pie,                \n",
            "and/or              and/or              and/or              \n",
            "brownies            browni              brownies            \n",
            "she                 she                 she                 \n",
            "didn't              didn't              didn't              \n",
            "like                like                like                \n",
            "the                 the                 the                 \n",
            "brownies            browni              brownies            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cyKCfY6QbO0r"
      },
      "source": [
        "# Vocabulary\n",
        "\n",
        "The code below obtains the vocabulary of the corpus. \n",
        "\n",
        "Q. Print the size of the vocabulary.\n",
        "\n",
        "Q. A programatically cleaner (and shorter) way of writing the code below by using a set instead of a list. Can you implement the code below using a set?\n",
        "\n",
        "##### 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9bWQ3zKYKlQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a8ea4327-f286-456e-fd92-32a3dad93fce"
      },
      "source": [
        "\"\"\"\n",
        "vocabulary = [] # Let us put all the tokens (mostly words) \n",
        "                # appearing in the vocabulary in a list\n",
        "  \n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "\"\"\"\n",
        "\n",
        "vocabulary = set()\n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        vocabulary.add(token)\n",
        "print(vocabulary)\n",
        "\n",
        "# Q. what is the size of the vocabulary?\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(f'Vocabulary size: {vocabulary_size}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'paris', 'he', 'and/or', 'will', 'woman', 'a', 'capital', 'france.', 'king', 'london', '...', \"didn't\", 'is', 'eat', 'is,', 'like', 'the', 'germany', 'brownies', 'pie,', 'she', 'england', 'cake,', 'man', 'of', 'berlin', 'queen'}\n",
            "Vocabulary size: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xGg6CtALe8Va"
      },
      "source": [
        "# Helper functions \n",
        "\n",
        "* These are some of the common helper functions that are used for NLP models:\n",
        "\n",
        "    * `word2idx`:  Maintains a dictionary of word and the corresponding index\n",
        "    \n",
        "    * `idx2word`: Maintains a mapping from index to word \n",
        "    \n",
        "    \n",
        "* Print the word2idx and idx2word, we will be using these in future exercises. \n",
        "\n",
        "##### 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LWut1gtXGQN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "990af8d5-a913-48d7-8ddb-e2e7e6db6169"
      },
      "source": [
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "print(word2idx)\n",
        "print(idx2word)\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'paris': 0, 'he': 1, 'and/or': 2, 'will': 3, 'woman': 4, 'a': 5, 'capital': 6, 'france.': 7, 'king': 8, 'london': 9, '...': 10, \"didn't\": 11, 'is': 12, 'eat': 13, 'is,': 14, 'like': 15, 'the': 16, 'germany': 17, 'brownies': 18, 'pie,': 19, 'she': 20, 'england': 21, 'cake,': 22, 'man': 23, 'of': 24, 'berlin': 25, 'queen': 26}\n",
            "{0: 'paris', 1: 'he', 2: 'and/or', 3: 'will', 4: 'woman', 5: 'a', 6: 'capital', 7: 'france.', 8: 'king', 9: 'london', 10: '...', 11: \"didn't\", 12: 'is', 13: 'eat', 14: 'is,', 15: 'like', 16: 'the', 17: 'germany', 18: 'brownies', 19: 'pie,', 20: 'she', 21: 'england', 22: 'cake,', 23: 'man', 24: 'of', 25: 'berlin', 26: 'queen'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1bFjK86NPChI"
      },
      "source": [
        "# Look-up table \n",
        "\n",
        "* This is a table that maps from an index to a one hot vector. \n",
        "\n",
        "Q. Print one-hot vectors corresponding to the words 'the', 'he' and ''england'\n",
        "\n",
        "##### 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vp8OTZI-UrYU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "88d438fc-51e9-4539-8de9-69fd5d6dd1f7"
      },
      "source": [
        "\n",
        "def look_up_table(word_idx):\n",
        "    x = torch.zeros(vocabulary_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x\n",
        "  \n",
        "# This is a one hot representation\n",
        "\n",
        "# Q. try printing it for word_idx = 1\n",
        "\n",
        "\n",
        "for word in ('he', 'the', 'england'):\n",
        "    word_idx = word2idx[word]\n",
        "    print(word, look_up_table(word_idx))\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "the tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "england tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PcAI5BtaQMFh"
      },
      "source": [
        "# Extracting contexts and the focus word\n",
        "\n",
        "\n",
        "Recall that we are building the skip-gram model. \n",
        "\n",
        "**We first begin by obtaining the set of contexts and focus words.**\n",
        "* Let's say we have a sentence (represented as vocabulary indicies): `[0, 2, 3, 6, 7]`.\n",
        "* For every word in the sentence, we want to get the words which are `window_size` around it.\n",
        "* So if `window_size==2`, for the word '0', we obtain: `[[0, 2], [0, 3]]`\n",
        "* For the word '2', we obtain: `[[2, 0], [2, 3], [2, 6]]`\n",
        "* For the word '3', we obtain: `[[3, 0], [3, 2], [3, 6], [3, 7]]`\n",
        "\n",
        "Q. Print some of the index pairs and trace them back to their words. \n",
        "\n",
        "\n",
        "##### 10 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wrtOwTUsyArb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9fa8a00-bf62-40ae-d911-5af5197dd0c6"
      },
      "source": [
        "window_size = 2\n",
        "\n",
        "idx_pairs = []\n",
        "\n",
        "# variables of interest: \n",
        "#   center_word_pos: center word position\n",
        "#   context_word_pos: context_word_position\n",
        "#   add sentence length as a constraint\n",
        "\n",
        "for sentence in tokenized_corpus:\n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "    \n",
        "    for center_word_pos in range(len(indices)):\n",
        "        \n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            \n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "                \n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
        "for center_idx, context_idx in idx_pairs:\n",
        "    print(idx2word[center_idx], idx2word[context_idx])\n",
        "\n",
        "# print(idx_pairs)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he is\n",
            "he a\n",
            "is he\n",
            "is a\n",
            "is king\n",
            "a he\n",
            "a is\n",
            "a king\n",
            "king is\n",
            "king a\n",
            "she is\n",
            "she a\n",
            "is she\n",
            "is a\n",
            "is queen\n",
            "a she\n",
            "a is\n",
            "a queen\n",
            "queen is\n",
            "queen a\n",
            "he is\n",
            "he a\n",
            "is he\n",
            "is a\n",
            "is man\n",
            "a he\n",
            "a is\n",
            "a man\n",
            "man is\n",
            "man a\n",
            "she is\n",
            "she a\n",
            "is she\n",
            "is a\n",
            "is woman\n",
            "a she\n",
            "a is\n",
            "a woman\n",
            "woman is\n",
            "woman a\n",
            "london is,\n",
            "london the\n",
            "is, london\n",
            "is, the\n",
            "is, capital\n",
            "the london\n",
            "the is,\n",
            "the capital\n",
            "the of\n",
            "capital is,\n",
            "capital the\n",
            "capital of\n",
            "capital england\n",
            "of the\n",
            "of capital\n",
            "of england\n",
            "england capital\n",
            "england of\n",
            "berlin is\n",
            "berlin ...\n",
            "is berlin\n",
            "is ...\n",
            "is the\n",
            "... berlin\n",
            "... is\n",
            "... the\n",
            "... capital\n",
            "the is\n",
            "the ...\n",
            "the capital\n",
            "the of\n",
            "capital ...\n",
            "capital the\n",
            "capital of\n",
            "capital germany\n",
            "of the\n",
            "of capital\n",
            "of germany\n",
            "germany capital\n",
            "germany of\n",
            "paris is\n",
            "paris the\n",
            "is paris\n",
            "is the\n",
            "is capital\n",
            "the paris\n",
            "the is\n",
            "the capital\n",
            "the of\n",
            "capital is\n",
            "capital the\n",
            "capital of\n",
            "capital france.\n",
            "of the\n",
            "of capital\n",
            "of france.\n",
            "france. capital\n",
            "france. of\n",
            "he will\n",
            "he eat\n",
            "will he\n",
            "will eat\n",
            "will cake,\n",
            "eat he\n",
            "eat will\n",
            "eat cake,\n",
            "eat pie,\n",
            "cake, will\n",
            "cake, eat\n",
            "cake, pie,\n",
            "cake, and/or\n",
            "pie, eat\n",
            "pie, cake,\n",
            "pie, and/or\n",
            "pie, brownies\n",
            "and/or cake,\n",
            "and/or pie,\n",
            "and/or brownies\n",
            "brownies pie,\n",
            "brownies and/or\n",
            "she didn't\n",
            "she like\n",
            "didn't she\n",
            "didn't like\n",
            "didn't the\n",
            "like she\n",
            "like didn't\n",
            "like the\n",
            "like brownies\n",
            "the didn't\n",
            "the like\n",
            "the brownies\n",
            "brownies like\n",
            "brownies the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rLzHkq6FRULl"
      },
      "source": [
        "# Parameters and hyperparameters \n",
        "\n",
        "* For our toy task, let us set the embedding dimensions to 5\n",
        "* Let us run the algorithm for 10 epochs (number of times the training algorithm looks at the corpus/training data)\n",
        "* Let us choose the learning rate as 0.001\n",
        "\n",
        "We have two parameter matrices $W_1$ and $W_2$ - the embedding matrix and the weight matrix. \n",
        "\n",
        "Q. What are the dimensionalities of $W_1$ and $W_2$?\n",
        "\n",
        "\n",
        "\n",
        "##### 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1kBhE6FeRuDu",
        "colab": {}
      },
      "source": [
        "# Hyperparameters:\n",
        "embedding_dims = 30\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "# The two weight matrices:\n",
        "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "W2 = torch.randn(vocabulary_size, embedding_dims, requires_grad=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "npzWXeQATPs4"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "(Refer to Lecture 2 slides 30-31)\n",
        "\n",
        "In the code below, we are going to compute the log probability of the correct context (target) given the word. \n",
        "\n",
        "Before running the code, answer the question commented in the code -> fill `y_true`.\n",
        "\n",
        "Print the loss and see if the loss goes down.\n",
        "\n",
        "###### 10 mins\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Cx2eJi1a8R6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "40883b8f-e982-4f49-ab82-632b817feb07"
      },
      "source": [
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  \n",
        "    loss_val = 0\n",
        "    \n",
        "    for data, target in idx_pairs:\n",
        "      \n",
        "        x = torch.Tensor(look_up_table(data)) #, requires_grad=True) # x is a One-hot tensor\n",
        "    \n",
        "        # Q. what would y_true be? \n",
        "        y_true = torch.Tensor([target]).long()\n",
        "\n",
        "        # A. \n",
        "        # 1-dimensional tensor of the index of the context word.\n",
        "\n",
        "        # \n",
        "        z1 = torch.matmul(W1, x) \n",
        "        # Q. what is z1? \n",
        "        # A.\n",
        "        # Word vector representation of x\n",
        "\n",
        "        z2 = torch.matmul(W2, z1)\n",
        "        # Q. what is the above operation? \n",
        "    \n",
        "        # Let us obtain prediction over the vocabulary\n",
        "        log_softmax = F.log_softmax(z2, dim=0)\n",
        "        \n",
        "        \n",
        "        # Our loss is a negative log-likelihood loss \n",
        "        # (what does this mean?)\n",
        "        \n",
        "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        \n",
        "        loss_val += loss.item()\n",
        "        \n",
        "        # propagate the error\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        # zero out gradient accumulation\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "\n",
        "print(f'\\nFinal epoch loss: {loss_val/len(idx_pairs)}')        "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:06<00:00, 16.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Final epoch loss: 1.790815689225695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JPsXq60HUWWq"
      },
      "source": [
        "Q. Given that we are interested in distributed representations, what is the major bottleneck in our setup? Is it the dimensionality of the representations? Is it the learning rate? Is it the corpus? \n",
        "\n",
        "> Corpus\n",
        "\n",
        "Q. What hyperparameters would you tune to improve the representations? \n",
        "\n",
        "> * Larger embedding dimension for a more expressive and flexible representation\n",
        "> * Decreasing learning rate...?\n",
        "> * More epochs for training\n",
        "\n",
        "Q. Train the algorithm with a bigger corpus. \n",
        "\n",
        "(You can either copy and paste the corpus and bring it to the same format as the corpus above or use the hint below)\n",
        "\n",
        "###### 10 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ixhvaYkW_SfX",
        "colab": {}
      },
      "source": [
        "# Example code for getting corpora from the internet\n",
        "import urllib\n",
        "txt = [line.strip() for line in urllib.request.urlopen('https://raw.githubusercontent.com/luonglearnstocode/Seinfeld-text-corpus/master/corpus.txt').readlines()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u2znwTJeU3UT"
      },
      "source": [
        "# Using word embeddings\n",
        "\n",
        "One of the simplest ways of exploiting word representations is to find similar words. There are many ways of measuring the semantic similarity between two words. As we are using word representations which are vectors in the euclidean space, distance metrics defined in the euclidean space are the most popular choice. This is because words that share common contexts in the corpus are located in close proximity to one another in the euclidean space.  One such metric is the eucldeian distance.\n",
        "\n",
        "Q. What is the euclidean distance between 'the' and 'a' (in the sample corpus and the new corpus)? \n",
        "\n",
        "Q. What other distance metrics can we use for two vectors? \n",
        "\n",
        "\n",
        "\n",
        "###### 10 mins\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NUPETZaOgvgB",
        "colab": {}
      },
      "source": [
        "# Let us get two vectors from the trained model\n",
        "\n",
        "x = torch.Tensor(look_up_table(0))\n",
        "x_emb = torch.matmul(W1, x).detach().numpy()\n",
        "y = torch.Tensor(look_up_table(1))\n",
        "y_emb = torch.matmul(W1, y).detach().numpy()\n",
        "\n",
        "# let us print the euclidean distance\n",
        "print(euclidean(x_emb, y_emb))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7UZroMvLtFVv"
      },
      "source": [
        "# ADVANCED: Training with negative sampling \n",
        " \n",
        " \n",
        " \n",
        "\n",
        "Refer to skipgram models in the slides. \n",
        "\n",
        "Q. What happens when we have a very large vocabulary? \n",
        "\n",
        "Q. What is a negative sample? \n",
        "\n",
        "\n",
        "Below is the code for training the model with negative sampling. \n",
        "\n",
        "\n",
        "##### 10 mins \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lq1lcWbqRwNY",
        "colab": {}
      },
      "source": [
        "# The two weight matrices:\n",
        "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "W2 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for data, target in idx_pairs:\n",
        "        x_var = Variable(look_up_table(data)).float() \n",
        "        \n",
        "        y_pos = Variable(torch.from_numpy(np.array([target])).long())\n",
        "        y_pos_var = Variable(look_up_table(target)).float()\n",
        "        \n",
        "        neg_sample = np.random.choice(list(range(vocabulary_size)),size=(1))[0]\n",
        "        y_neg = Variable(torch.from_numpy(np.array([neg_sample])))\n",
        "        y_neg_var = Variable(look_up_table(neg_sample)).float()\n",
        "         \n",
        "        x_emb = torch.matmul(W1, x_var) \n",
        "        y_pos_emb = torch.matmul(W2, y_pos_var)\n",
        "        y_neg_emb = torch.matmul(W2, y_neg_var)\n",
        "        \n",
        "        # get positive sample score\n",
        "        pos_loss = F.logsigmoid(torch.matmul(x_emb, y_pos_emb))\n",
        "        \n",
        "        # get negsample score\n",
        "        neg_loss = F.logsigmoid(-1 * torch.matmul(x_emb, y_neg_emb))\n",
        "        \n",
        "        loss = - (pos_loss + neg_loss)\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        # propagate the error\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        # zero out gradient accumulation\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "        \n",
        "    if epoch % 10 == 0:    \n",
        "        print(f'Loss at epo {epoch}: {epoch_loss/len(idx_pairs)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGgcB8VHXvPK"
      },
      "source": [
        "* In the current setup, we are only exploiting a very small sample of negative examples. This is suboptimal. \n",
        "\n",
        "* Given a sufficiently large vocabulary, we would ideally sample the negative samples from a noise distribution whose probabilities match the frequency of vocabulary.\n",
        "\n",
        "\n",
        "Q.  Using this code as the basis, build an object oriented negative sampling based model and train it on the fairly large corpus. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y_Eknuozunq5",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f4VCX4XSqvlu"
      },
      "source": [
        "# Pre-trained representations\n",
        "\n",
        "We have seen from the above that word embeddings are learned in an unsupervised manner, i.e., we don't have any labelled data. These representations can be used to `bootstrap' models in NLP. There are many word representation inducing algorithms : [word2vec](https://arxiv.org/abs/1301.3781), [GloVe](https://nlp.stanford.edu/pubs/glove.pdf), [Fasttext](https://arxiv.org/abs/1607.04606) are some of the popular choices. There are differences in the algorithms but they are all based on the distributional hypothesis. \n",
        "\n",
        "We will now use one of these pre-trained representations: GloVe. \n",
        "\n",
        "Q. What is the dimensionality of the representations below? \n",
        "\n",
        "##### 2 mins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OCsV8mtBg4-Y",
        "colab": {}
      },
      "source": [
        "w2i = [] # word2index\n",
        "i2w = [] # index2word\n",
        "wvecs = [] # word vectors\n",
        "\n",
        "# this is a large file, it will take a while to load in the memory!\n",
        "with codecs.open('glove.6B.50d.txt', 'r','utf-8') as f: \n",
        "  index = 0\n",
        "  for line in tqdm(f.readlines()):\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
        "    if len(line.strip().split()) > 3:\n",
        "      \n",
        "      (word, vec) = (line.strip().split()[0], \n",
        "                     list(map(float,line.strip().split()[1:]))) \n",
        "      \n",
        "      wvecs.append(vec)\n",
        "      w2i.append((word, index))\n",
        "      i2w.append((index, word))\n",
        "      index += 1\n",
        "\n",
        "w2i = dict(w2i)\n",
        "i2w = dict(i2w)\n",
        "wvecs = np.array(wvecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2cP-UmR5Br4_"
      },
      "source": [
        "For the following experiments, we recommend  using `wvecs` - the pretrained representations. \n",
        "\n",
        "# Evaluating word representation models\n",
        "\n",
        "## Inrtinsic Evaluation\n",
        "\n",
        "* Intrinsic evaluation of word representations involves evaluating  set of word vectors generated by an embedding technique on specific  subtasks that in someways are directly related to the distributional hypothesis. These are typically simple and fast to compute and thereby allow us to help understand representation learning algorithms.\n",
        "\n",
        "* An intrinsic evaluation should typically return to us a scalar quantity that measures the performance of those word vectors on the evaluation subtask.\n",
        "\n",
        "\n",
        "\n",
        "## Word Similarity\n",
        "\n",
        "The first task we consider is evaluating if the representations are good at computing if two words are similar. In this task, you will use both euclidean distance or cosine distance as similarity measures. \n",
        "\n",
        "* Print similarity scores for word pairs in https://github.com/iraleviant/eval-multilingual-simlex/blob/master/evaluation/ws-353/wordsim353-english-sim.txt\n",
        "\n",
        "     (Format of the file: two words and the corresponding human score for the two words)\n",
        "\n",
        "* Obtain pearson's correlation with predicted scores and the human generated scores. \n",
        "\n",
        "\n",
        "##### 15 mins\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S8Kev8-wA1QA"
      },
      "source": [
        "##  Exploring Analogies\n",
        "\n",
        "The second task we consider **completing analogies**. We are given an incomplete analogy of the form: \n",
        "\n",
        "\n",
        "* $a : b : : c :~?$\n",
        "\n",
        "\n",
        "We would then identify the word vector which maximizes the cosine similarity. \n",
        "This metric has an intuitive interpretation. Ideally, we want $\\phi(b) - \\phi(a) = \\phi(d) - \\phi(c)$ where $\\phi(.)$ is the word vector. \n",
        "For instance, \n",
        "\n",
        "* *london $-$ england = paris $-$ france* .\n",
        "\n",
        "Thus we identify the vector $\\phi(d)$ which maximizes the normalized dot-product between the two word\n",
        "vectors (i.e. cosine similarity).\n",
        "\n",
        "\n",
        "\n",
        "* You can either use your own method to compute the correct word or use the code below. \n",
        "\n",
        "* Use original analogies dataset https://github.com/svn2github/word2vec/blob/master/questions-words.txt \n",
        "\n",
        "Q. When does it fail? \n",
        "\n",
        "Q. What are the possible reasons for failure?\n",
        "\n",
        "##### 15mins\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWVu38ePjh-i",
        "colab": {}
      },
      "source": [
        "def cosine_distance(u, v):\n",
        "    distance = 0.0\n",
        "    dot = np.dot(u,v)\n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "    distance = dot/(norm_u)/norm_v\n",
        "    return distance\n",
        "  \n",
        " \n",
        "def find_analogy(word_a, word_b, word_c, word_vectors, word2index):\n",
        "    word_a = word_a.lower()\n",
        "    word_b = word_b.lower()\n",
        "    word_c = word_c.lower()\n",
        "    \n",
        "    (e_a, e_b, e_c) = (word_vectors[word2index[word_a]], \n",
        "                       word_vectors[word2index[word_b]], \n",
        "                       word_vectors[word2index[word_c]])\n",
        "    \n",
        "    \n",
        "    max_cosine_sim = -999\n",
        "    best_word = None\n",
        "    \n",
        "    for (w, i) in word2index.items():\n",
        "        if w in [word_a, word_b, word_c]:\n",
        "            continue\n",
        "        cosine_sim = cosine_distance(e_b - e_a, word_vectors[i] - e_c)\n",
        "        \n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = w\n",
        "            \n",
        "    return best_word\n",
        "  \n",
        "# find_analogy('france', 'paris', 'england', wvecs, w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mqchbyIiCOcQ"
      },
      "source": [
        "# Advanced: Compositionality \n",
        "\n",
        "* Given access to only word representations, how can we build representations for phrases and sentences? \n",
        "\n",
        "  (Hint: algebraic operation is one way) \n",
        "\n",
        "\n",
        "* Compute the similarity score between two sentences on the STS.input.MSRpar.txt dataset from https://github.com/alvations/stasis/tree/master/STS-data/STS2012-train \n",
        "\n",
        "  (Please use 00-readme.txt in the corpus for details on the format)\n",
        "  \n",
        "* Measure the pearson correlation with the human scores in STS.gs.MSRpar.txt\n",
        "\n",
        "Q. What problems did you encounter when computing the scores? \n",
        "\n",
        "Q. What are alternative ways of computing the scores? \n",
        "\n",
        "Q. Using your composition method, compute representations for the following expressions and also list the top-5 most similar words: \n",
        "\n",
        "* New York \n",
        "* kick the bucket\n",
        "* post office\n",
        "\n",
        "  Does it work? What are the possible reasons? \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qUwxb7CPt_n3"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "* [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf): word2vec reference\n",
        "\n",
        "* [Eluciating the properties of semantic word representations](http://www.offconvex.org/2016/02/14/word-embeddings-1/): A global perspective\n",
        "\n",
        "* [Understanding the algebraic notions of semantic word representations](http://www.offconvex.org/2015/12/12/word-embeddings-1/): Why does the word-analogies task work with simple algebraic manipulations?\n",
        "\n",
        "* [Stemming And Lemmatization](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)"
      ]
    }
  ]
}